{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d32b4731-e363-4aaa-b43e-a0c52305bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b37a59f-8638-403e-bd3f-10e6f61eeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257,     # Vocabulary size\n",
    "\"context_length\": 256,  # Context length\n",
    "\"emb_dim\": 768,          # Embedding dimension\n",
    "\"n_heads\": 12,           # Number of attnention heads\n",
    "\"n_layers\": 12,          # Number of layers\n",
    "\"drop_rate\": 0.1,        # Dropout rate\n",
    "\"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230074ce-3b0c-4637-b370-d8b3fa27198f",
   "metadata": {},
   "outputs": [],
   "source": [
    " class LayerNorm(nn.Module):          \n",
    "    def __init__(self, emb_dim):   \n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "                                  \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515f21b0-48bb-4108-a518-5ee8eb8b4714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7118f3a0-1d93-4dda-aa8d-d11e25cf6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59faf9ea-5f51-47d4-be4e-35a5208ba7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f50ad5c8-517d-4e3c-9c92-61f2113b30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c31d131-fa60-4834-83f9-af350b3907a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out=layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c602055a-aab8-4401-b281-9c13f13167c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d25c310c-4b26-435b-a43c-d0039a08ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "        (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8bf938e0-ab37-4d39-9d20-e65008734d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "805f6d38-8725-4a0d-ab59-9dc8a5e9f24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 4,722,432\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "total_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9469ea0b-ea03-4beb-b91c-4b76abdf9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "        context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)        \n",
    "        queries = self.W_query(x)   \n",
    "        values = self.W_value(x) \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)                                                                   \n",
    "        keys = keys.transpose(1, 2)         \n",
    "        queries = queries.transpose(1, 2)   \n",
    "        values = values.transpose(1, 2)     \n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]   \n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)    \n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(\"Attn weights shape \", attn_weights.shape)\n",
    "        print(\"Values weights shape \", values.shape)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  \n",
    "        print(\"Context shape \", context_vec.shape)\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec = self.out_proj(context_vec)   \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0526b915-22a8-4d50-b12c-89c6e04de431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 7,085,568\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "db6dc4d6-a399-47fe-b4fb-e53a44f9df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "34725371-f975-46b3-9c0b-ca105b939df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                  \n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b208acd4-928c-4e96-a7d1-e8f3531eb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_block = nn.Sequential(\n",
    "            * [TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c28797dd-da52-441b-ba06-fb526e19e907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape  torch.Size([2, 4])\n",
      "x shape  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "tok_emb = nn.Embedding(50257, 768)\n",
    "x = tok_emb(batch)\n",
    "print(\"batch shape \",batch.shape)\n",
    "print(\"x shape \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f959aae0-d526-4c0a-980f-d600ba0bc3fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3290880344.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[142], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    torch.arange(4, device = x.dev\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "torch.arange(4, device = x.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "135714d3-6e9f-4c50-9127-61f755ad4d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Attn weights shape  torch.Size([2, 12, 4, 4])\n",
      "Values weights shape  torch.Size([2, 12, 4, 64])\n",
      "Context shape  torch.Size([2, 4, 12, 64])\n",
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.4708,  0.5737, -0.5967,  ...,  0.2019, -0.5665,  0.1800],\n",
      "         [-0.3895, -0.1978, -0.8885,  ...,  0.2242, -1.2341,  0.1752],\n",
      "         [ 0.6973, -0.3432, -0.6080,  ...,  0.3747, -0.6967,  0.1088],\n",
      "         [-0.2962, -0.6957, -1.1371,  ...,  0.3579,  0.3058, -0.2915]],\n",
      "\n",
      "        [[-0.1514,  0.3329, -0.9740,  ..., -0.1368, -0.6974, -0.1851],\n",
      "         [-0.4894, -0.3492, -0.9759,  ...,  0.2951, -0.3396,  0.2109],\n",
      "         [ 0.5082, -0.1425,  0.2549,  ...,  0.1618,  0.1304, -0.3092],\n",
      "         [-0.4146, -0.0514, -0.5187,  ..., -0.1869, -0.1303, -0.4969]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c5c5270c-dd4b-4c23-a204-5784d51a6a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a826760e-292d-40f2-9d19-419168a6cf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    " print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    " print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f9a616f-e549-4571-bfed-9eb4faa10631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 123,822,336\n"
     ]
    }
   ],
   "source": [
    " total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    " )\n",
    " print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba05b78-3a98-48c1-91ab-312dbfbc7b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,   \n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,      \n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]   \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx,idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)               \n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb693ea8-9d93-496e-a058-4f3f060649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "               [40,    1107, 588]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65563dff-4f28-49f7-b1e2-30c89e2351af",
   "metadata": {},
   "outputs": [],
   "source": [
    " targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                [1107, 588, 11311]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16f0c8ab-a58f-435c-9518-392945cb3202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    print(logits.shape)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e6bff78-b49e-4408-871c-31a7fc4b32d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(token_ids.shape)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15553fe-5d65-4f06-ac50-0e3504d8976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1685a5d-bfb4-4571-a9be-ff945afa09b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(targets.shape)\n",
    "print(targets[0].shape)\n",
    "print(token_ids[0].shape)\n",
    "print(token_ids[0].flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e63a65b-7f37-424f-b4e6-d372fbe6b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs batch 1: Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Outputs batch 1:\" f\"{token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dcf82d8-9292-4890-a4e7-bc4b006bbeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eee136c5-d5e7-47eb-ae1c-6edb14672d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    " print(\"Logits shape:\", logits.shape)\n",
    " print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d990cc1-47f5-4de2-9292-c8c4ecb9e842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50257])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0,1)\n",
    "targets_flat = targets.flatten()\n",
    "print(logits_flat.shape)\n",
    "print(targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "585fc616-7785-4ce4-8648-ca1f814537a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c17396c4-ce47-4277-81ae-e8a581bde80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"sample-text.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "624d5031-cdaa-4320-ae5a-898878238799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ff76410-d9a7-4b30-a923-4ac808de33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7cd8822-cab9-427a-ad17-503928177adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):    \n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):   \n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):        \n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "152623b3-7152-461c-94d0-8c29e4a6e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "             stride=128, shuffle=True, drop_last=True,\n",
    "             num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")                        \n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  \n",
    "    dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    drop_last=drop_last,    \n",
    "    num_workers=num_workers    \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16ea24fa-947b-489d-ae75-f081dbf3ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    "    )\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d78b5538-b1ea-4b78-bae2-2fc6cbb16de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "9\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "print(len(train_loader))\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51fa98a4-1595-45de-9472-51046aaae64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f9a38609c40>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffd5fb50-f7c2-4655-8f3d-3f87262e9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)        \n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "    logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84b5387a-8bb1-4508-a764-076bf95a5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "         num_batches = len(data_loader)    \n",
    "    else:\n",
    "         num_batches = min(num_batches, len(data_loader))  \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a56c32e9-3915-4ad8-8347-e131a0b23932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  10.987583266364204\n",
      "Validation loss:  10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss: \", train_loss)\n",
    "print(\"Validation loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e500288-94db-439b-be27-7fb0601f2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}) : \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model,tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "                \n",
    "                \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e5e7027-3b4d-45f5-ad39-60cd231996e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval() \n",
    "    with torch.no_grad():                             \n",
    "        train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed5be4ef-31aa-4dc9-9c50-fa218525721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "10223912-9c98-46d1-b48b-853138156c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000) : Train loss 3.743, Val loss 3.560\n",
      "Ep 1 (Step 000005) : Train loss 3.819, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 2 (Step 000010) : Train loss 3.762, Val loss 3.560\n",
      "Ep 2 (Step 000015) : Train loss 3.632, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 3 (Step 000020) : Train loss 3.711, Val loss 3.560\n",
      "Ep 3 (Step 000025) : Train loss 3.729, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 4 (Step 000030) : Train loss 3.714, Val loss 3.560\n",
      "Ep 4 (Step 000035) : Train loss 3.765, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 5 (Step 000040) : Train loss 3.750, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 6 (Step 000045) : Train loss 3.796, Val loss 3.560\n",
      "Ep 6 (Step 000050) : Train loss 3.757, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 7 (Step 000055) : Train loss 3.683, Val loss 3.560\n",
      "Ep 7 (Step 000060) : Train loss 3.718, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 8 (Step 000065) : Train loss 3.666, Val loss 3.560\n",
      "Ep 8 (Step 000070) : Train loss 3.650, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 9 (Step 000075) : Train loss 3.803, Val loss 3.560\n",
      "Ep 9 (Step 000080) : Train loss 3.763, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n",
      "Ep 10 (Step 000085) : Train loss 3.793, Val loss 3.560\n",
      "Every effort moves you forward.  The first step is to understand the importance of your work.  The second step is to understand the importance of your work.  The third step is to understand the importance of your work.  The fourth step is\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "#model = GPTModel(GPT_CONFIG_124M)\n",
    "gpt.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    gpt, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs = num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bdcc77ee-be68-40b4-927e-cf8fd3cda669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "    epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                  \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "128f02cd-7521-4ebd-bd6e-b368d9889d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=25,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ca4fa60-e4de-414b-936f-54a66d69a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits/temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42244dd9-9910-41ff-8615-680cfde18817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know began to happen. It was not it was such not to see her\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a34ecdf9-bf24-4c9f-b1c8-2c38ed2c663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(), \n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "},\n",
    "           \"model_and_optimizer.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc919964-c5e1-4838-8e59-e86bc37a1964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x7f990916f070>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch05/\"\n",
    "\"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5eae6af8-1d21-4213-ad27-8cc672d88a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-15 17:49:36.072625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-15 17:49:36.858800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 117kiB/s]\n",
      "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:01<00:00, 652kiB/s]\n",
      "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 117kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [05:46<00:00, 1.44MiB/s]\n",
      "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 7.42MiB/s]\n",
      "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:01<00:00, 305kiB/s]\n",
      "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:01<00:00, 352kiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1babb3f6-97b6-4391-b3a3-7fe980ed09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9055a143-5b8f-4015-9c28-99f3976f9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    " print(params[\"wte\"])\n",
    " print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6329f15-6791-44e2-91e3-25fd636c3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bab99141-55d1-4e12-89ce-b914128ce906",
   "metadata": {},
   "outputs": [],
   "source": [
    " model_name = \"gpt2-small (124M)\"\n",
    " NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    " NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ac74f07-3296-41b8-bde3-c109d7b83581",
   "metadata": {},
   "outputs": [],
   "source": [
    " NEW_CONFIG.update({\"context_length\": 1024})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2428a425-1968-4bb3-8087-e6d901229804",
   "metadata": {},
   "outputs": [],
   "source": [
    " NEW_CONFIG.update({\"qkv_bias\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "90de21fc-e383-4489-9f0c-f9b1e09cf7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_block): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " gpt = GPTModel(NEW_CONFIG)\n",
    " gpt.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3ea49943-68c1-4691-8490-5a1680b12855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right:{right.shape}\"\n",
    "         )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "804e043d-2496-41d6-b591-caab8b66b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):          \n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    for b in range(len(params[\"blocks\"])):    \n",
    "        q_w, k_w, v_w = np.split(                           \n",
    "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_block[b].attn.W_query.weight = assign(\n",
    "        gpt.trf_block[b].attn.W_query.weight, q_w.T)\n",
    "        gpt.trf_block[b].attn.W_key.weight = assign(\n",
    "        gpt.trf_block[b].attn.W_key.weight, k_w.T)\n",
    "        gpt.trf_block[b].attn.W_value.weight = assign(\n",
    "        gpt.trf_block[b].attn.W_value.weight, v_w.T)\n",
    "        q_b, k_b, v_b = np.split(\n",
    "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_block[b].attn.W_query.bias = assign(\n",
    "        gpt.trf_block[b].attn.W_query.bias, q_b)\n",
    "        gpt.trf_block[b].attn.W_key.bias = assign(\n",
    "        gpt.trf_block[b].attn.W_key.bias, k_b)\n",
    "        gpt.trf_block[b].attn.W_value.bias = assign(\n",
    "        gpt.trf_block[b].attn.W_value.bias, v_b)\n",
    "        gpt.trf_block[b].attn.out_proj.weight = assign(\n",
    "        gpt.trf_block[b].attn.out_proj.weight, \n",
    "        params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_block[b].attn.out_proj.bias = assign(\n",
    "        gpt.trf_block[b].attn.out_proj.bias, \n",
    "        params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "        gpt.trf_block[b].ff.layers[0].weight = assign(\n",
    "        gpt.trf_block[b].ff.layers[0].weight, \n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_block[b].ff.layers[0].bias = assign(\n",
    "        gpt.trf_block[b].ff.layers[0].bias, \n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_block[b].ff.layers[2].weight = assign(\n",
    "        gpt.trf_block[b].ff.layers[2].weight, \n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_block[b].ff.layers[2].bias = assign(\n",
    "        gpt.trf_block[b].ff.layers[2].bias, \n",
    "        params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "        gpt.trf_block[b].norm1.scale = assign(\n",
    "        gpt.trf_block[b].norm1.scale, \n",
    "        params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_block[b].norm1.shift = assign(\n",
    "        gpt.trf_block[b].norm1.shift, \n",
    "        params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_block[b].norm2.scale = assign(\n",
    "        gpt.trf_block[b].norm2.scale, \n",
    "        params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_block[b].norm2.shift = assign(\n",
    "        gpt.trf_block[b].norm2.shift, \n",
    "        params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67e6294b-ac4b-4185-9d8e-dc229bb15ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_block): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfc18d28-7723-4179-a9fa-ea1e376dc67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far the road can travel because I was told that with 100% recovery between breaths you'll lose as you move upwards.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "model=gpt,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "max_new_tokens=25,\n",
    "context_size=NEW_CONFIG[\"context_length\"],\n",
    "top_k=50,\n",
    "temperature=3\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861fc6f4-e9c0-4e42-907f-e307c66a9c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
